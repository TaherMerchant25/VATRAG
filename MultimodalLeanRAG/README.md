# Multimodal LeanRAG

Knowledge-Graph-Based Generation with Semantic Aggregation for Multimodal Audio Data.

This module extends the LeanRAG framework to handle multimodal data, specifically audio content. It extracts audio chunks, stores them in Milvus vector database with unique signature IDs, and builds a knowledge graph to represent relationships between audio segments.

## ğŸ¯ Key Features

- **Audio Chunking**: Split audio files into overlapping chunks with unique signature IDs
- **Vector Embeddings**: Generate embeddings using CLAP or simple mel-spectrogram features
- **Milvus Storage**: Store chunk embeddings for fast similarity search
- **Knowledge Graph**: Store signature IDs as nodes with relationship edges
- **Hierarchical Retrieval**: Bottom-up traversal from chunks to aggregations

## ğŸ—ï¸ Architecture

```
Audio File
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Audio Chunking                  â”‚
â”‚  - Split into 10s chunks            â”‚
â”‚  - 2s overlap (sliding window)      â”‚
â”‚  - Generate unique signature IDs    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Embedding Generation            â”‚
â”‚  - CLAP model (512-dim)             â”‚
â”‚  - Or simple mel-spectrogram        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Milvus Vector Store             â”‚
â”‚  - Store embeddings with metadata   â”‚
â”‚  - Map signature_id â†’ vector        â”‚
â”‚  - Enable similarity search         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Knowledge Graph                 â”‚
â”‚  - Nodes: signature IDs             â”‚
â”‚  - Edges: chunk relationships       â”‚
â”‚    â€¢ Sequential (temporal)          â”‚
â”‚    â€¢ Semantic similarity            â”‚
â”‚    â€¢ Aggregation hierarchy          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Module Structure

```
MultimodalLeanRAG/
â”œâ”€â”€ config.yaml           # Configuration file
â”œâ”€â”€ requirements.txt      # Dependencies
â”œâ”€â”€ audio_chunking.py     # Audio file chunking with signature ID generation
â”œâ”€â”€ audio_embedding.py    # CLAP/mel-spectrogram embeddings
â”œâ”€â”€ milvus_store.py       # Milvus vector database operations
â”œâ”€â”€ knowledge_graph.py    # SQLite-based knowledge graph
â”œâ”€â”€ pipeline.py           # Main pipeline orchestrator
â”œâ”€â”€ query.py              # Retrieval and query module
â””â”€â”€ README.md             # This file
```

## ğŸš€ Quick Start

### Installation

```bash
cd MultimodalLeanRAG
pip install -r requirements.txt
```

### Basic Usage

#### 1. Process Audio Files

```python
from pipeline import MultimodalLeanRAGPipeline, PipelineConfig

# Configure pipeline
config = PipelineConfig(
    working_dir="./output",
    chunk_duration_sec=10.0,
    overlap_sec=2.0,
    embedding_model="simple"  # or "clap" for better quality
)

# Initialize and run
pipeline = MultimodalLeanRAGPipeline(config)

# Process single file
results = pipeline.process_audio_file("path/to/audio.wav")

# Or process directory
results = pipeline.process_audio_directory("path/to/audio_folder/")
```

#### 2. Query the Index

```python
from query import MultimodalRetriever

retriever = MultimodalRetriever(working_dir="./output")

# Search by text (requires CLAP model)
results = retriever.search_by_text("speech about machine learning")

# Get hierarchical context
context = retriever.get_hierarchical_context(
    signature_ids=[r.signature_id for r in results]
)
```

### Command Line Interface

```bash
# Process audio
python pipeline.py --input ./audio_files --output-dir ./output

# Query
python query.py --working-dir ./output --query "What topics are discussed?"
```

## ğŸ”‘ Signature ID System

Each audio chunk receives a unique signature ID based on:
- Audio content hash (first/last 1000 samples)
- Source file name
- Chunk index and timing

Format: `audio_{hash16}`

Example: `audio_a1b2c3d4e5f67890`

## ğŸ“Š Knowledge Graph Schema

### Nodes Table
| Field | Type | Description |
|-------|------|-------------|
| signature_id | TEXT (PK) | Unique chunk identifier |
| node_type | TEXT | 'audio_chunk' or 'aggregation' |
| level | INTEGER | Hierarchy level (0 = base) |
| description | TEXT | Generated description |
| parent_id | TEXT (FK) | Parent aggregation node |
| source_file | TEXT | Original audio file |
| metadata | JSON | Additional info |

### Edges Table
| Field | Type | Description |
|-------|------|-------------|
| source_id | TEXT (FK) | Source node |
| target_id | TEXT (FK) | Target node |
| relation_type | TEXT | Edge type |
| weight | REAL | Relationship strength |
| description | TEXT | Relation description |

### Relation Types
- `sequential` - Temporally adjacent chunks
- `semantic_similar` - Similar audio content
- `same_speaker` - Same speaker detected
- `same_topic` - Same topic/theme
- `aggregation` - Chunk to aggregation node
- `hierarchy` - Parent-child relationship

## âš™ï¸ Configuration

Edit `config.yaml`:

```yaml
# Audio processing
audio:
  chunk_duration_sec: 10
  overlap_sec: 2
  sample_rate: 16000

# Embeddings
embeddings:
  audio:
    model: "laion/clap-htsat-unfused"  # or "simple"
    dimension: 512

# Knowledge Graph
knowledge_graph:
  use_sqlite: true

# Processing
processing:
  similarity_threshold: 0.7
  batch_size: 32
```

## ğŸ”„ Pipeline Flow

```
1. Load audio file(s)
           â†“
2. Chunk into segments with overlap
           â†“
3. Generate signature ID for each chunk (SHA256 hash)
           â†“
4. Generate embedding vector (CLAP or mel-spectrogram)
           â†“
5. Store in Milvus: {signature_id, embedding, metadata}
           â†“
6. Create KG node: {signature_id, type, level, description}
           â†“
7. Create KG edges:
   - Sequential: chunk[i] â†’ chunk[i+1]
   - Semantic: high similarity pairs
           â†“
8. Ready for retrieval!
```

## ğŸ” Retrieval Strategy

Implements LeanRAG-style hierarchical retrieval:

1. **Vector Search**: Find top-K similar chunks via Milvus
2. **Graph Expansion**: Traverse KG edges to find related chunks
3. **Bottom-Up**: Walk up hierarchy to aggregation nodes
4. **Reasoning Paths**: Find paths connecting retrieved chunks
5. **Context Building**: Compile multi-level context for LLM

## ğŸ“ Example Output

```json
{
  "signature_id": "audio_a1b2c3d4e5f67890",
  "source_file": "podcast_episode_1.mp3",
  "chunk_index": 5,
  "start_time": 40.0,
  "end_time": 50.0,
  "duration": 10.0,
  "relations": [
    {"type": "sequential", "target": "audio_b2c3d4e5f6789012"},
    {"type": "semantic_similar", "target": "audio_c3d4e5f67890123a", "weight": 0.85}
  ]
}
```

## ğŸ§ª Testing

```bash
# Test audio chunking
python audio_chunking.py --input test.wav --output chunks.json

# Test Milvus store
python milvus_store.py

# Test knowledge graph
python knowledge_graph.py

# Full pipeline test
python pipeline.py --input test_audio/ --output-dir ./test_output
```

## ğŸ“š Related

- [LeanRAG](../README.md) - Main LeanRAG framework
- [CLAP](https://github.com/LAION-AI/CLAP) - Contrastive Language-Audio Pretraining
- [Milvus](https://milvus.io/) - Vector database

## ğŸ“„ License

MIT License - See main repository LICENSE file.
